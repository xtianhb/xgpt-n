{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e39ca4-0d12-4067-a76a-e5c9f2e594a2",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0725d424-18f8-4bf0-93cb-b5ea5ca00429",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"data/quijote.txt\"\n",
    "with open(file_path, \"r\", encoding='utf-8') as fd:\n",
    "    text = fd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da22dcac-0b38-4246-9538-260feb3d814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of the dataset in characters: 2128954\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lenght of the dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a2c3e9-2ef2-4351-8dc3-ef84c1187ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DON QUIJOTE DE LA MANCHA\n",
      "Miguel de Cervantes Saavedra\n",
      "\n",
      "PRIMERA PARTE\n",
      "CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. \n",
      "Quijote de la Mancha\n",
      "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho \n",
      "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, \n",
      "rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, \n",
      "salpicón las más noches, duelos y quebrantos los sábados, lentejas los \n",
      "viernes, algún palomino de añadidura los domingos, consumían las tres \n",
      "partes de su hacienda. El resto della concluían sayo de velarte, calzas de \n",
      "velludo para las fiestas con sus pantuflos de lo mismo, los días de entre \n",
      "semana se honraba con su vellori de lo más fino. Tenía en su casa una ama \n",
      "que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un \n",
      "mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. \n",
      "Frisaba la edad de nuestro hidalgo con los cincuenta años, era de \n",
      "complexión recia, seco de ca\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bc7126-772c-48ed-aa5c-ad2ab7057854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters set: \n",
      " !\"(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvxyz¡«»¿ÁÉÍÚáéíñóúü–\n",
      "Vocabulary size: 88\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(\"Characters set: \" + \"\".join(chars))\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087fa636-4f54-4b04-84aa-db35055ce249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 67, 52, 60, 61, 65, 1, 51, 82, 48, 65, 1, 40, 48, 60, 50, 55, 61]\n",
      "Buenos días Sancho\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"Buenos días Sancho\"))\n",
    "print(decode(encode(\"Buenos días Sancho\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9812f41d-b646-4416-869d-40ec953a46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2128954]) torch.int64\n",
      "tensor([25, 36, 35,  1, 38, 42, 30, 31, 36, 41, 26,  1, 25, 26,  1, 33, 22,  1,\n",
      "        34, 22, 35, 24, 29, 22,  0, 34, 56, 54, 67, 52, 58,  1, 51, 52,  1, 24,\n",
      "        52, 64, 68, 48, 60, 66, 52, 65,  1, 40, 48, 48, 68, 52, 51, 64, 48,  0,\n",
      "         0, 37, 39, 30, 34, 26, 39, 22,  1, 37, 22, 39, 41, 26,  0, 24, 22, 37,\n",
      "        78, 41, 42, 33, 36,  1, 10, 19,  1, 38, 67, 52,  1, 66, 64, 48, 66, 48,\n",
      "         1, 51, 52,  1, 58, 48,  1, 50, 61, 60, 51, 56, 50, 56, 84, 60,  1, 70,\n",
      "         1, 52, 57, 52, 64, 50, 56, 50, 56, 61,  1, 51, 52, 58,  1, 53, 48, 59,\n",
      "        61, 65, 61,  1, 55, 56, 51, 48, 58, 54, 61,  1, 25,  8,  1,  0, 38, 67,\n",
      "        56, 57, 61, 66, 52,  1, 51, 52,  1, 58, 48,  1, 34, 48, 60, 50, 55, 48,\n",
      "         0, 26, 60,  1, 67, 60,  1, 58, 67, 54, 48, 64,  1, 51, 52,  1, 58, 48,\n",
      "         1, 34, 48, 60, 50, 55, 48,  6,  1, 51, 52,  1, 50, 67, 70, 61,  1, 60,\n",
      "        61, 59, 49, 64, 52,  1, 60, 61,  1, 63, 67, 56, 52, 64, 61,  1, 48, 50,\n",
      "        61, 64, 51, 48, 64, 59, 52,  6,  1, 60, 61,  1, 55, 48,  1, 59, 67, 50,\n",
      "        55, 61,  1,  0, 66, 56, 52, 59, 62, 61,  1, 63, 67, 52,  1, 68, 56, 68,\n",
      "        82, 48,  1, 67, 60,  1, 55, 56, 51, 48, 58, 54, 61,  1, 51, 52,  1, 58,\n",
      "        61, 65,  1, 51, 52,  1, 58, 48, 60, 71, 48,  1, 52, 60,  1, 48, 65, 66,\n",
      "        56, 58, 58, 52, 64, 61,  6,  1, 48, 51, 48, 64, 54, 48,  1, 48, 60, 66,\n",
      "        56, 54, 67, 48,  6,  1,  0, 64, 61, 50, 82, 60,  1, 53, 58, 48, 50, 61,\n",
      "         1, 70,  1, 54, 48, 58, 54, 61,  1, 50, 61, 64, 64, 52, 51, 61, 64,  8,\n",
      "         1, 42, 60, 48,  1, 61, 58, 58, 48,  1, 51, 52,  1, 48, 58, 54, 61,  1,\n",
      "        59, 80, 65,  1, 68, 48, 50, 48,  1, 63, 67, 52,  1, 50, 48, 64, 60, 52,\n",
      "        64, 61,  6,  1,  0, 65, 48, 58, 62, 56, 50, 84, 60,  1, 58, 48, 65,  1,\n",
      "        59, 80, 65,  1, 60, 61, 50, 55, 52, 65,  6,  1, 51, 67, 52, 58, 61, 65,\n",
      "         1, 70,  1, 63, 67, 52, 49, 64, 48, 60, 66, 61, 65,  1, 58, 61, 65,  1,\n",
      "        65, 80, 49, 48, 51, 61, 65,  6,  1, 58, 52, 60, 66, 52, 57, 48, 65,  1,\n",
      "        58, 61, 65,  1,  0, 68, 56, 52, 64, 60, 52, 65,  6,  1, 48, 58, 54, 85,\n",
      "        60,  1, 62, 48, 58, 61, 59, 56, 60, 61,  1, 51, 52,  1, 48, 83, 48, 51,\n",
      "        56, 51, 67, 64, 48,  1, 58, 61, 65,  1, 51, 61, 59, 56, 60, 54, 61, 65,\n",
      "         6,  1, 50, 61, 60, 65, 67, 59, 82, 48, 60,  1, 58, 48, 65,  1, 66, 64,\n",
      "        52, 65,  1,  0, 62, 48, 64, 66, 52, 65,  1, 51, 52,  1, 65, 67,  1, 55,\n",
      "        48, 50, 56, 52, 60, 51, 48,  8,  1, 26, 58,  1, 64, 52, 65, 66, 61,  1,\n",
      "        51, 52, 58, 58, 48,  1, 50, 61, 60, 50, 58, 67, 82, 48, 60,  1, 65, 48,\n",
      "        70, 61,  1, 51, 52,  1, 68, 52, 58, 48, 64, 66, 52,  6,  1, 50, 48, 58,\n",
      "        71, 48, 65,  1, 51, 52,  1,  0, 68, 52, 58, 58, 67, 51, 61,  1, 62, 48,\n",
      "        64, 48,  1, 58, 48, 65,  1, 53, 56, 52, 65, 66, 48, 65,  1, 50, 61, 60,\n",
      "         1, 65, 67, 65,  1, 62, 48, 60, 66, 67, 53, 58, 61, 65,  1, 51, 52,  1,\n",
      "        58, 61,  1, 59, 56, 65, 59, 61,  6,  1, 58, 61, 65,  1, 51, 82, 48, 65,\n",
      "         1, 51, 52,  1, 52, 60, 66, 64, 52,  1,  0, 65, 52, 59, 48, 60, 48,  1,\n",
      "        65, 52,  1, 55, 61, 60, 64, 48, 49, 48,  1, 50, 61, 60,  1, 65, 67,  1,\n",
      "        68, 52, 58, 58, 61, 64, 56,  1, 51, 52,  1, 58, 61,  1, 59, 80, 65,  1,\n",
      "        53, 56, 60, 61,  8,  1, 41, 52, 60, 82, 48,  1, 52, 60,  1, 65, 67,  1,\n",
      "        50, 48, 65, 48,  1, 67, 60, 48,  1, 48, 59, 48,  1,  0, 63, 67, 52,  1,\n",
      "        62, 48, 65, 48, 49, 48,  1, 51, 52,  1, 58, 61, 65,  1, 50, 67, 48, 64,\n",
      "        52, 60, 66, 48,  6,  1, 70,  1, 67, 60, 48,  1, 65, 61, 49, 64, 56, 60,\n",
      "        48,  1, 63, 67, 52,  1, 60, 61,  1, 58, 58, 52, 54, 48, 49, 48,  1, 48,\n",
      "         1, 58, 61, 65,  1, 68, 52, 56, 60, 66, 52,  6,  1, 70,  1, 67, 60,  1,\n",
      "         0, 59, 61, 71, 61,  1, 51, 52,  1, 50, 48, 59, 62, 61,  1, 70,  1, 62,\n",
      "        58, 48, 71, 48,  6,  1, 63, 67, 52,  1, 48, 65, 82,  1, 52, 60, 65, 56,\n",
      "        58, 58, 48, 49, 48,  1, 52, 58,  1, 64, 61, 50, 82, 60,  1, 50, 61, 59,\n",
      "        61,  1, 66, 61, 59, 48, 49, 48,  1, 58, 48,  1, 62, 61, 51, 48, 51, 52,\n",
      "        64, 48,  8,  1,  0, 27, 64, 56, 65, 48, 49, 48,  1, 58, 48,  1, 52, 51,\n",
      "        48, 51,  1, 51, 52,  1, 60, 67, 52, 65, 66, 64, 61,  1, 55, 56, 51, 48,\n",
      "        58, 54, 61,  1, 50, 61, 60,  1, 58, 61, 65,  1, 50, 56, 60, 50, 67, 52,\n",
      "        60, 66, 48,  1, 48, 83, 61, 65,  6,  1, 52, 64, 48,  1, 51, 52,  1,  0,\n",
      "        50, 61, 59, 62, 58, 52, 69, 56, 84, 60,  1, 64, 52, 50, 56, 48,  6,  1,\n",
      "        65, 52, 50, 61,  1, 51, 52,  1, 50, 48])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036515cc-6f21-4be7-9593-057932ee12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c94646b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 36, 35,  1, 38, 42, 30, 31, 36])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#context_lenght\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4899c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([25]) the target: 36\n",
      "when input is tensor([25, 36]) the target: 35\n",
      "when input is tensor([25, 36, 35]) the target: 1\n",
      "when input is tensor([25, 36, 35,  1]) the target: 38\n",
      "when input is tensor([25, 36, 35,  1, 38]) the target: 42\n",
      "when input is tensor([25, 36, 35,  1, 38, 42]) the target: 30\n",
      "when input is tensor([25, 36, 35,  1, 38, 42, 30]) the target: 31\n",
      "when input is tensor([25, 36, 35,  1, 38, 42, 30, 31]) the target: 36\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d090d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[52, 65, 62, 52, 64, 48, 60, 71],\n",
      "        [66, 52, 60, 54, 48,  1, 49, 64],\n",
      "        [67, 60, 66, 48, 59, 52, 60, 66],\n",
      "        [ 1, 38, 67, 56, 57, 61, 66, 52]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[65, 62, 52, 64, 48, 60, 71, 48],\n",
      "        [52, 60, 54, 48,  1, 49, 64, 82],\n",
      "        [60, 66, 48, 59, 52, 60, 66, 52],\n",
      "        [38, 67, 56, 57, 61, 66, 52,  6]])\n",
      "----\n",
      "when input is [52] the target: 65\n",
      "when input is [52, 65] the target: 62\n",
      "when input is [52, 65, 62] the target: 52\n",
      "when input is [52, 65, 62, 52] the target: 64\n",
      "when input is [52, 65, 62, 52, 64] the target: 48\n",
      "when input is [52, 65, 62, 52, 64, 48] the target: 60\n",
      "when input is [52, 65, 62, 52, 64, 48, 60] the target: 71\n",
      "when input is [52, 65, 62, 52, 64, 48, 60, 71] the target: 48\n",
      "when input is [66] the target: 52\n",
      "when input is [66, 52] the target: 60\n",
      "when input is [66, 52, 60] the target: 54\n",
      "when input is [66, 52, 60, 54] the target: 48\n",
      "when input is [66, 52, 60, 54, 48] the target: 1\n",
      "when input is [66, 52, 60, 54, 48, 1] the target: 49\n",
      "when input is [66, 52, 60, 54, 48, 1, 49] the target: 64\n",
      "when input is [66, 52, 60, 54, 48, 1, 49, 64] the target: 82\n",
      "when input is [67] the target: 60\n",
      "when input is [67, 60] the target: 66\n",
      "when input is [67, 60, 66] the target: 48\n",
      "when input is [67, 60, 66, 48] the target: 59\n",
      "when input is [67, 60, 66, 48, 59] the target: 52\n",
      "when input is [67, 60, 66, 48, 59, 52] the target: 60\n",
      "when input is [67, 60, 66, 48, 59, 52, 60] the target: 66\n",
      "when input is [67, 60, 66, 48, 59, 52, 60, 66] the target: 52\n",
      "when input is [1] the target: 38\n",
      "when input is [1, 38] the target: 67\n",
      "when input is [1, 38, 67] the target: 56\n",
      "when input is [1, 38, 67, 56] the target: 57\n",
      "when input is [1, 38, 67, 56, 57] the target: 61\n",
      "when input is [1, 38, 67, 56, 57, 61] the target: 66\n",
      "when input is [1, 38, 67, 56, 57, 61, 66] the target: 52\n",
      "when input is [1, 38, 67, 56, 57, 61, 66, 52] the target: 6\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Get rand ints between 0 and len(data) - block_size, with a 1d array batch_size\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd8fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[52, 65, 62, 52, 64, 48, 60, 71],\n",
      "        [66, 52, 60, 54, 48,  1, 49, 64],\n",
      "        [67, 60, 66, 48, 59, 52, 60, 66],\n",
      "        [ 1, 38, 67, 56, 57, 61, 66, 52]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f49478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 88])\n",
      "tensor(5.0107, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "¡)Á?:-4 émGDLG8eD5L¡.TOZNPRjT¡aáJázóE»3¡Ú.\n",
      "PRgéQ3OsKRVf5MUl¿A–Ú-ytñNV7B(-4H3ih1Ú.:f;3rpj6c,ÉHSOrnsx»\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d4d8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c41b1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2918145656585693\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be8d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mannzPió¡sto, \n",
      "centajososereni sen fane pestr; hodontide KClel mes pe y irandoquere meúbama; ve pun delo n idida potrogo.\n",
      "Die to ie lzachal den de ha quvien mu la y lldebodos a s quozcren y quen Quenase Cuentundan \n",
      "is doreza pañxtrrtadi de qunijenue pe sendagondesYÁü¿Qun iciKCa a sitangos co, co mela \n",
      "lviroRha ses e me o; lorenta erile lanco l y e paros ve viés ngastrtos ciferr hade e deado y dele\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-xgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
